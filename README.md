# Smart Multimodal ADAS ğŸš—ğŸ“¡ğŸ“·ğŸ¤–

An end-to-end **Smart Multimodal Advanced Driver Assistance System (ADAS)** prototype combining **Radar**, **Camera**, **Sensor Fusion**, **Tracking**, **Reinforcement Learningâ€“based Decision Making**, and **LLM Explainability**.

This project demonstrates a **research-grade ADAS pipeline** built step-by-step with clean modular design and explainable decisions.

---

## ğŸš€ Features

- ğŸ“¡ **Radar Processing**
  - RadarScenes dataset ingestion
  - DBSCAN-based radar point clustering
  - Kalman-filter multi-object tracking

- ğŸ“· **Camera Perception**
  - YOLOv8 object detection
  - GPU-accelerated inference (GTX 1650 Ti supported)

- ğŸ”— **Sensor Fusion**
  - Temporal alignment (radar â†” camera)
  - Radar-first spatial association
  - Confidence-based fusion logic

- ğŸ§­ **Multi-Object Tracking**
  - Fused radarâ€“camera tracking
  - Track lifecycle management (creation, update, deletion)

- ğŸ® **Decision Making**
  - Custom Gym-based ADAS environment
  - Deep Q-Network (DQN) policy
  - Safety-aware reward shaping

- ğŸ§  **Explainable AI (XAI)**
  - Decision logging (JSONL)
  - Local LLM (Ollama) explanations
  - Human-readable safety reasoning

---

## ğŸ–¼ï¸ Visual Results & Demo

### Camera-Based Object Detection (YOLOv8)
The system detects vehicles and road objects using camera perception.

![Camera Detection](demo/camera_detection_1.jpg)

---

### Radar-Based Tracking
Radar point clustering and Kalman-filter-based tracking of surrounding objects.

![Radar Tracking](demo/radar_tracking.png)

---

## ğŸ—‚ï¸ Project Structure

```text
smart-multimodal-adas/
â”‚
â”œâ”€â”€ RadarScenes/                # Dataset (external)
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ sequence_1/
â”‚
â”œâ”€â”€ demo/                       # Outputs & trained models
â”‚   â”œâ”€â”€ camera_radarscenes/
â”‚   â””â”€â”€ adas_dqn_model.zip
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ decision_log.jsonl      # RL decision logs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ sensors/
â”‚   â”‚   â””â”€â”€ radarscenes_loader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ perception/
â”‚   â”‚   â”œâ”€â”€ camera_detect_radarscenes.py
â”‚   â”‚   â””â”€â”€ legacy/
â”‚   â”‚
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â”œâ”€â”€ fusion_logic.py
â”‚   â”‚   â””â”€â”€ legacy/
â”‚   â”‚
â”‚   â”œâ”€â”€ tracking/
â”‚   â”‚   â”œâ”€â”€ fused_tracker.py
â”‚   â”‚   â”œâ”€â”€ run_fused_tracking.py
â”‚   â”‚   â””â”€â”€ test_fused_tracking.py
â”‚   â”‚
â”‚   â”œâ”€â”€ decision/
â”‚   â”‚   â”œâ”€â”€ adas_env.py
â”‚   â”‚   â”œâ”€â”€ train_rl_agent.py
â”‚   â”‚   â”œâ”€â”€ test_rl_agent.py
â”‚   â”‚   â”œâ”€â”€ decision_logger.py
â”‚   â”‚   â””â”€â”€ legacy/
â”‚   â”‚
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ explain_decision.py
â”‚
â””â”€â”€ README.md

```

---

## âš™ï¸ Environment Setup

### Requirements

* **Python:** 3.10
* **Hardware:** NVIDIA GPU (optional, tested on GTX 1650 Ti)
* **Package Manager:** Conda (recommended)

### 1. Create Conda Environment

```bash
conda create -n smadas python=3.10 -y
conda activate smadas
```

### 2. Install Dependencies

```bash
pip install torch torchvision torchaudio
pip install ultralytics open3d h5py opencv-python matplotlib scikit-learn
pip install gym gymnasium stable-baselines3 shimmy
```

---

## ğŸ“Š Dataset

This project uses the **RadarScenes** dataset.

* **Website:** [radar-scenes.com](https://radar-scenes.com/)
* **Kaggle:** [RadarScenes Data Set](https://www.kaggle.com/datasets/aleksandrdubrovin/the-radarscenes-data-set)

> **Note:** After downloading, ensure the following path exists: `RadarScenes/data/sequence_1/`

## â–¶ï¸ Execution Order (Important)

Run the scripts in the following order:

### 1ï¸âƒ£ Radar Processing & Tracking

```bash
python src/sensors/radarscenes_loader.py
```

### 2ï¸âƒ£ Camera Object Detection

```bash
python src/perception/camera_detect_radarscenes.py
```

### 3ï¸âƒ£ Temporal Alignment

```bash
python src/fusion/temporal_alignment.py
```

### 4ï¸âƒ£ Radarâ€“Camera Fusion

```bash
python src/fusion/fusion_logic.py
```

### 5ï¸âƒ£ Fused Multi-Object Tracking

```bash
python src/tracking/test_fused_tracking.py
```

### 6ï¸âƒ£ Train RL Decision Agent

```bash
python src/decision/train_rl_agent.py
```

### 7ï¸âƒ£ Test RL Agent + Log Decisions

```bash
python src/decision/test_rl_agent.py
```

> Logs will be saved to: `logs/decision_log.jsonl`

### 8ï¸âƒ£ Explain Decisions with LLM

*(Requires Ollama running locally)*

```bash
python src/llm/explain_decision.py
```

---

## ğŸ§  Example LLM Explanation

```text
 "The vehicle applied braking because the detected object was within a safe stopping distance. Sensor fusion confidence was high, indicating reliable perception. This action reduced collision risk. A future improvement would be incorporating object velocity."
```

## ğŸ§  Explainable Decision Making (LLM)

Each safety-critical decision made by the RL agent is logged and explained
using a local Large Language Model (LLM).

The explanation includes:
- Why the action was chosen
- Whether it was safe
- Suggestions for improvement

![LLM Explanation](demo/llm_explanation.png)

---

## ğŸ§ª Technical Highlights
- DBSCAN clustering on radar point clouds
- Kalman-filter multi-target tracking
- Confidence-based radarâ€“camera fusion
- DQN-based decision policy
- Fully local explainability (no cloud APIs)

---

## ğŸ”® Future Work
- Proper radarâ€“camera calibration
- Velocity-aware RL state
- BEV visualization
- Multi-sequence training
- Transformer-based sensor fusion
- Formal safety envelopes

---

## ğŸ“œ License
This project is intended for research and educational use.

---

## ğŸ™Œ Acknowledgements
- RadarScenes Dataset authors
- Ultralytics YOLO
- Stable-Baselines3
- OpenAI Gym / Gymnasium
- Ollama
- 
---
